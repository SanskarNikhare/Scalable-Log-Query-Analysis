{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Task Category' column not found. Skipping drop operation.\n",
      "Error: 'Level' column not found. Skipping row filtering operation.\n",
      "Error: 'Level' or 'Original Message' column not found. Skipping row filtering operation.\n",
      "Error: 'Level' column not found. Skipping 'threshold' column creation.\n",
      "The modified CSV file has been saved successfully to: C:\\Users\\nikha\\OneDrive\\Desktop\\newtry\\modified_system.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file with ISO-8859-1 encoding\n",
    "file_path = r'C:\\Users\\nikha\\OneDrive\\Desktop\\newtry\\system.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Rename the last column to 'Original Message'\n",
    "df.columns.values[-1] = 'Original Message'\n",
    "\n",
    "# Drop the 'Task Category' column if it exists\n",
    "if 'Task Category' in df.columns:\n",
    "    df = df.drop(columns=['Task Category'])\n",
    "else:\n",
    "    print(\"'Task Category' column not found. Skipping drop operation.\")\n",
    "\n",
    "# Drop rows with 'Level' as \"Information\" or \"Warning\"\n",
    "if 'Level' in df.columns:\n",
    "    df = df[~df['Level'].isin(['Information', 'Warning'])]\n",
    "else:\n",
    "    print(\"Error: 'Level' column not found. Skipping row filtering operation.\")\n",
    "\n",
    "# List of keywords related to success or status success\n",
    "keywords = ['success', 'successful', 'status success', 'operation successful', 'operation success',\n",
    "            'completed successfully', 'successfully completed', 'task success', 'process success', \n",
    "            'execution success', 'execution successful', 'transaction success']\n",
    "\n",
    "# Create a regular expression pattern from the keywords\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "# Remove rows where 'Level' is \"Information\" and 'Original Message' contains any of the keywords (case insensitive)\n",
    "if 'Level' in df.columns and 'Original Message' in df.columns:\n",
    "    df = df[~((df['Level'] == 'Information') & df['Original Message'].str.contains(pattern, case=False))]\n",
    "else:\n",
    "    print(\"Error: 'Level' or 'Original Message' column not found. Skipping row filtering operation.\")\n",
    "\n",
    "# Replace special characters like :,;'\"<>./?\\| with a single space in 'Original Message'\n",
    "df['Original Message'] = df['Original Message'].apply(lambda x: re.sub(r'[:;\\'-_\"<>./?\\\\|]', ' ', str(x)))\n",
    "\n",
    "# Remove duplicate messages while keeping the first occurrence\n",
    "df = df.drop_duplicates(subset=['Original Message'], keep='first')\n",
    "\n",
    "# Add a 'threshold' column based on the 'Level' values\n",
    "if 'Level' in df.columns:\n",
    "    df['Threshold'] = df['Level'].apply(lambda x: 0.8 if x == 'Error' else (0.6 if x == 'Critical' else None))\n",
    "else:\n",
    "    print(\"Error: 'Level' column not found. Skipping 'threshold' column creation.\")\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "output_path = r'C:\\Users\\nikha\\OneDrive\\Desktop\\newtry\\modified_system.csv'\n",
    "try:\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"The modified CSV file has been saved successfully to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_1.csv\n",
      "Chunk 2 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_2.csv\n",
      "Chunk 3 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_3.csv\n",
      "Chunk 4 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_4.csv\n",
      "Chunk 5 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_5.csv\n",
      "Chunk 6 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_6.csv\n",
      "Chunk 7 saved as C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\\log_chunk_7.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# File path for the preprocessed log file\n",
    "file_path = r\"C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\Updated Methods\\modified_system.csv\"\n",
    "\n",
    "# Desired chunk size in bytes (1 KB = 1024 bytes)\n",
    "chunk_size = 1024  \n",
    "\n",
    "# Output directory for chunks\n",
    "output_folder = r\"C:\\Users\\nikha\\OneDrive\\Desktop\\FyI Project\\Preprocessing\\newtry\\Chunk_Output\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Open the input file and start splitting\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)  # Read the header row\n",
    "    \n",
    "    file_number = 1\n",
    "    current_chunk_size = 0\n",
    "    chunk_rows = []  # To store rows for the current chunk\n",
    "\n",
    "    for row in reader:\n",
    "        # Calculate the size of the current row\n",
    "        row_size = sum(len(str(item).encode(\"utf-8\")) for item in row) + len(row)  # Include commas\n",
    "        \n",
    "        # Check if adding this row exceeds the chunk size\n",
    "        if current_chunk_size + row_size > chunk_size:\n",
    "            # Write the current chunk to a file\n",
    "            chunk_file_path = os.path.join(output_folder, f\"log_chunk_{file_number}.csv\")\n",
    "            with open(chunk_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as chunk_file:\n",
    "                writer = csv.writer(chunk_file)\n",
    "                writer.writerow(header)  # Write the header to maintain structure\n",
    "                writer.writerows(chunk_rows)  # Write the rows of this chunk\n",
    "            \n",
    "            print(f\"Chunk {file_number} saved as {chunk_file_path}\")\n",
    "            file_number += 1\n",
    "            chunk_rows = []  # Reset rows for the new chunk\n",
    "            current_chunk_size = 0\n",
    "        \n",
    "        # Add the current row to the chunk\n",
    "        chunk_rows.append(row)\n",
    "        current_chunk_size += row_size\n",
    "    \n",
    "    # Write the remaining rows as the last chunk\n",
    "    if chunk_rows:\n",
    "        chunk_file_path = os.path.join(output_folder, f\"log_chunk_{file_number}.csv\")\n",
    "        with open(chunk_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as chunk_file:\n",
    "            writer = csv.writer(chunk_file)\n",
    "            writer.writerow(header)  # Write the header\n",
    "            writer.writerows(chunk_rows)  # Write the remaining rows\n",
    "        \n",
    "        print(f\"Chunk {file_number} saved as {chunk_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
